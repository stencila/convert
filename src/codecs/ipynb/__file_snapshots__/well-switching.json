{
  "type": "Article",
  "title": "Untitled",
  "authors": [],
  "meta": {
    "name": "arsenic_wells_switching"
  },
  "content": [
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 1
      },
      "text": "%pylab inline",
      "outputs": [
        "\nWelcome to pylab, a matplotlib-based Python environment [backend: module://IPython.zmq.pylab.backend_inline].\nFor more information, type 'help(pylab)'.\n"
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 2
      },
      "text": "import numpy as np\nfrom pandas import *\nfrom statsmodels.formula.api import logit\nfrom statsmodels.nonparametric import KDE\nimport matplotlib.pyplot as plt\nfrom patsy import dmatrix, dmatrices"
    },
    {
      "type": "Heading",
      "depth": 1,
      "content": [
        "Logistic models of well switching in Bangladesh"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Our data are information on about 3,000 respondent households in Bangladesh with wells having an unsafe amount of arsenic. The data record the amount of arsenic in the respondent's well, the distance to the nearest safe well (in meters), whether that respondent \"switched\" wells by using a neighbor's safe well instead of their own, as well as the respondent's years of education and a dummy variable indicating whether they belong to a community association."
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Our goal is to model well-switching decision. Since it's a binary variable (1 = switch, 0 = no switch), we'll use logistic regression."
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "This analysis follows Gelman and Hill ",
        {
          "type": "Emphasis",
          "content": [
            "Data Analysis Using Regression and Multilevel/Hierarchical Models"
          ]
        },
        ", chapter 5.4. "
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 19
      },
      "text": "df = read_csv('data/wells.dat', sep = ' ', header = 0, index_col = 0)\nprint df.head()",
      "outputs": [
        "   switch  arsenic       dist  assoc  educ\n1       1     2.36  16.826000      0     0\n2       1     0.71  47.321999      0     0\n3       0     2.07  20.966999      0    10\n4       1     1.15  21.486000      0    12\n5       1     1.10  40.874001      1    14\n"
      ]
    },
    {
      "type": "Heading",
      "depth": 2,
      "content": [
        "Model 1: Distance to a safe well"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "For our first pass, we'll just use the distance to the nearest safe well. Since the distance is recorded in meters, and the effect of one meter is likely to be very small, we can get nicer model coefficients if we scale it. Instead of creating a new scaled variable, we'll just do it in the formula description using the ",
        {
          "type": "Code",
          "text": "I()"
        },
        " function."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 4
      },
      "text": "model1 = logit('switch ~ I(dist/100.)', df = df).fit()\nprint model1.summary()",
      "outputs": [
        "Optimization terminated successfully.\n         Current function value: 2038.118913\n         Iterations 4\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 switch   No. Observations:                 3020\nModel:                          Logit   Df Residuals:                     3018\nMethod:                           MLE   Df Model:                            1\nDate:                Sat, 22 Dec 2012   Pseudo R-squ.:                 0.01017\nTime:                        13:05:25   Log-Likelihood:                -2038.1\nconverged:                       True   LL-Null:                       -2059.0\n                                        LLR p-value:                 9.798e-11\n==================================================================================\n                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n----------------------------------------------------------------------------------\nIntercept          0.6060      0.060     10.047      0.000         0.488     0.724\nI(dist / 100.)    -0.6219      0.097     -6.383      0.000        -0.813    -0.431\n==================================================================================\n"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Let's plot this model. We'll want to jitter the ",
        {
          "type": "Code",
          "text": "switch"
        },
        " data, since it's all 0/1 and will over-plot. "
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 5
      },
      "text": "def binary_jitter(x, jitter_amount = .05):\n    '''\n    Add jitter to a 0/1 vector of data for plotting.\n    '''\n    jitters = np.random.rand(*x.shape) * jitter_amount\n    x_jittered = x + np.where(x == 1, -1, 1) * jitters\n    return x_jittered"
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 6
      },
      "text": "dist_logit_par = model1.params['I(dist / 100.)']\nplt.plot(df['dist'], binary_jitter(df['switch'], .1), '.', alpha = .1)\nplt.plot(np.sort(df['dist']), model1.predict()[np.argsort(df['dist'])], lw = 2)\nplt.ylabel('Switched Wells')\nplt.xlabel('Distance from safe well (meters)')",
      "outputs": [
        {
          "type": "ImageObject",
          "format": "image/png",
          "contentUrl": ""
        }
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Another way to look at this is to plot the densities of distance for switchers and non-switchers. We expect the distribution of switchers to have more mass over short distances and the distribution of non-switchers to have more mass over long distances."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 7
      },
      "text": "kde_sw = KDE(df['dist'][df['switch'] == 1])\nkde_nosw = KDE(df['dist'][df['switch'] == 0])\n\nkde_sw.fit()\nkde_nosw.fit()\n\nplt.plot(kde_sw.support, kde_sw.density, label = 'Switch')\nplt.plot(kde_nosw.support, kde_nosw.density, color = 'red', label = 'No Switch')\nplt.xlabel('Distance (meters)')\nplt.legend(loc = 'best')",
      "outputs": [
        {
          "type": "ImageObject",
          "format": "image/png",
          "contentUrl": ""
        }
      ]
    },
    {
      "type": "Heading",
      "depth": 2,
      "content": [
        "Model 2: Distance to a safe well and the arsenic level of own well"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Next, let's add the arsenic level as a regressor. We'd expect respondents with higher arsenic levels to be more motivated to switch."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 8
      },
      "text": "model2 = logit('switch ~ I(dist / 100.) + arsenic', df = df).fit()\nprint model2.summary()",
      "outputs": [
        "Optimization terminated successfully.\n         Current function value: 1965.334134\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 switch   No. Observations:                 3020\nModel:                          Logit   Df Residuals:                     3017\nMethod:                           MLE   Df Model:                            2\nDate:                Sat, 22 Dec 2012   Pseudo R-squ.:                 0.04551\nTime:                        13:05:29   Log-Likelihood:                -1965.3\nconverged:                       True   LL-Null:                       -2059.0\n                                        LLR p-value:                 1.995e-41\n==================================================================================\n                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n----------------------------------------------------------------------------------\nIntercept          0.0027      0.079      0.035      0.972        -0.153     0.158\nI(dist / 100.)    -0.8966      0.104     -8.593      0.000        -1.101    -0.692\narsenic            0.4608      0.041     11.134      0.000         0.380     0.542\n==================================================================================\n"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Which is what we see. The coefficients are what we'd expect: the farther to a safe well, the less likely a respondent is to switch, but the higher the arsenic level in their own well, the more likely."
      ]
    },
    {
      "type": "Heading",
      "depth": 3,
      "content": [
        "Marginal effects"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "To see the effect of these on the probability of switching, let's calculate the marginal effects at the mean of the data."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 9
      },
      "text": "model2.margeff(at = 'mean')",
      "outputs": [
        "array([-0.21806505,  0.11206108])"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "So, for the mean respondent, an increase of 100 meters to the nearest safe well is associated with a 22% lower probability of switching. But an increase of 1 in the arsenic level is associated with an 11% higher probability of switching."
      ]
    },
    {
      "type": "Heading",
      "depth": 3,
      "content": [
        "Class separability"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "To get a sense of how well this model might classify switchers and non-switchers, we can plot each class of respondent in (distance-arsenic)-space. "
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "We don't see very clean separation, so we'd expect the model to have a fairly high error rate. But we do notice that the short-distance/high-arsenic region of the graph is mostly comprised switchers, and the long-distance/low-arsenic region is mostly comprised of non-switchers."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 10
      },
      "text": "logit_pars = model2.params\nintercept = -logit_pars[0] / logit_pars[2]\nslope = -logit_pars[1] / logit_pars[2]\n\ndist_sw = df['dist'][df['switch'] == 1]\ndist_nosw = df['dist'][df['switch'] == 0]\narsenic_sw = df['arsenic'][df['switch'] == 1]\narsenic_nosw = df['arsenic'][df['switch'] == 0]\nplt.figure(figsize = (12, 8))\nplt.plot(dist_sw, arsenic_sw, '.', mec = 'purple', mfc = 'None', \n         label = 'Switch')\nplt.plot(dist_nosw, arsenic_nosw, '.', mec = 'orange', mfc = 'None', \n         label = 'No switch')\nplt.plot(np.arange(0, 350, 1), intercept + slope * np.arange(0, 350, 1) / 100.,\n         '-k', label = 'Separating line')\nplt.ylim(0, 10)\nplt.xlabel('Distance to safe well (meters)')\nplt.ylabel('Arsenic level')\nplt.legend(loc = 'best')",
      "outputs": [
        {
          "type": "ImageObject",
          "format": "image/png",
          "contentUrl": ""
        }
      ]
    },
    {
      "type": "Heading",
      "depth": 2,
      "content": [
        "Model 3: Adding an interation"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "It's sensible that distance and arsenic would interact in the model. In other words, the effect of an 100 meters on your decision to switch would be affected by how much arsenic is in your well. "
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Again, we don't have to pre-compute an explicit interaction variable. We can just specify an interaction in the formula description using the ",
        {
          "type": "Code",
          "text": ":"
        },
        " operator."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 11
      },
      "text": "model3 = logit('switch ~ I(dist / 100.) + arsenic + I(dist / 100.):arsenic', \n                   df = df).fit()\nprint model3.summary()",
      "outputs": [
        "Optimization terminated successfully.\n         Current function value: 1963.814202\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 switch   No. Observations:                 3020\nModel:                          Logit   Df Residuals:                     3016\nMethod:                           MLE   Df Model:                            3\nDate:                Sat, 22 Dec 2012   Pseudo R-squ.:                 0.04625\nTime:                        13:05:33   Log-Likelihood:                -1963.8\nconverged:                       True   LL-Null:                       -2059.0\n                                        LLR p-value:                 4.830e-41\n==========================================================================================\n                             coef    std err          z      P>|z|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------------------\nIntercept                 -0.1479      0.118     -1.258      0.208        -0.378     0.083\nI(dist / 100.)            -0.5772      0.209     -2.759      0.006        -0.987    -0.167\narsenic                    0.5560      0.069      8.021      0.000         0.420     0.692\nI(dist / 100.):arsenic    -0.1789      0.102     -1.748      0.080        -0.379     0.022\n==========================================================================================\n"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "The coefficient on the interaction is negative and significant. While we can't directly intepret its quantitative effect on switching, the qualitative interpretation gels with our intuition. Distance has a negative effect on switching, but this negative effect is reduced when arsenic levels are high. Alternatively, the arsenic level have a positive effect on switching, but this positive effect is reduced as distance to the nearest safe well increases."
      ]
    },
    {
      "type": "Heading",
      "depth": 2,
      "content": [
        "Model 4: Adding educuation, more interactions and centering variables"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Respondents with more eduction might have a better understanding of the harmful effects of arsenic and therefore may be more likely to switch. Education is in years, so we'll scale it for more sensible coefficients. We'll also include interactions amongst all the regressors."
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "We're also going to center the variables, to help with interpretation of the coefficients. Once more, we can just do this in the formula, without pre-computing centered variables."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 12
      },
      "text": "model_form = ('switch ~ center(I(dist / 100.)) + center(arsenic) + ' +\n              'center(I(educ / 4.)) + ' +\n              'center(I(dist / 100.)) : center(arsenic) + ' + \n              'center(I(dist / 100.)) : center(I(educ / 4.)) + ' + \n              'center(arsenic) : center(I(educ / 4.))'\n             )\nmodel4 = logit(model_form, df = df).fit()\nprint model4.summary()",
      "outputs": [
        "Optimization terminated successfully.\n         Current function value: 1945.871775\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 switch   No. Observations:                 3020\nModel:                          Logit   Df Residuals:                     3013\nMethod:                           MLE   Df Model:                            6\nDate:                Sat, 22 Dec 2012   Pseudo R-squ.:                 0.05497\nTime:                        13:05:35   Log-Likelihood:                -1945.9\nconverged:                       True   LL-Null:                       -2059.0\n                                        LLR p-value:                 4.588e-46\n===============================================================================================================\n                                                  coef    std err          z      P>|z|      [95.0% Conf. Int.]\n---------------------------------------------------------------------------------------------------------------\nIntercept                                       0.3563      0.040      8.844      0.000         0.277     0.435\ncenter(I(dist / 100.))                         -0.9029      0.107     -8.414      0.000        -1.113    -0.693\ncenter(arsenic)                                 0.4950      0.043     11.497      0.000         0.411     0.579\ncenter(I(educ / 4.))                            0.1850      0.039      4.720      0.000         0.108     0.262\ncenter(I(dist / 100.)):center(arsenic)         -0.1177      0.104     -1.137      0.256        -0.321     0.085\ncenter(I(dist / 100.)):center(I(educ / 4.))     0.3227      0.107      3.026      0.002         0.114     0.532\ncenter(arsenic):center(I(educ / 4.))            0.0722      0.044      1.647      0.100        -0.014     0.158\n===============================================================================================================\n"
      ]
    },
    {
      "type": "Heading",
      "depth": 3,
      "content": [
        "Model assessment: Binned Residual plots"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "Plotting residuals to regressors can alert us to issues like nonlinearity or heteroskedasticity. Plotting raw residuals in a binary model isn't usually informative, so we do some smoothing. Here, we'll averaging the residuals within bins of the regressor. (A lowess or moving average might also work.)"
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 14
      },
      "text": "def bin_residuals(resid, var, bins):\n    '''\n    Compute average residuals within bins of a variable.\n    \n    Returns a dataframe indexed by the bins, with the bin midpoint,\n    the residual average within the bin, and the confidence interval \n    bounds.\n    '''\n    resid_df = DataFrame({'var': var, 'resid': resid})\n    resid_df['bins'] = qcut(var, bins)\n    bin_group = resid_df.groupby('bins')\n    bin_df = bin_group['var', 'resid'].mean()\n    bin_df['count'] = bin_group['resid'].count()\n    bin_df['lower_ci'] = -2 * (bin_group['resid'].std() / \n                               np.sqrt(bin_group['resid'].count()))\n    bin_df['upper_ci'] =  2 * (bin_group['resid'].std() / \n                               np.sqrt(bin_df['count']))\n    bin_df = bin_df.sort('var')\n    return(bin_df)\n\ndef plot_binned_residuals(bin_df):\n    '''\n    Plotted binned residual averages and confidence intervals.\n    '''\n    plt.plot(bin_df['var'], bin_df['resid'], '.')\n    plt.plot(bin_df['var'], bin_df['lower_ci'], '-r')\n    plt.plot(bin_df['var'], bin_df['upper_ci'], '-r')\n    plt.axhline(0, color = 'gray', lw = .5)\n    \narsenic_resids = bin_residuals(model4.resid, df['arsenic'], 40)\ndist_resids = bin_residuals(model4.resid, df['dist'], 40)\nplt.figure(figsize = (12, 5))\nplt.subplot(121)\nplt.ylabel('Residual (bin avg.)')\nplt.xlabel('Arsenic (bin avg.)')\nplot_binned_residuals(arsenic_resids)\nplt.subplot(122)\nplot_binned_residuals(dist_resids)\nplt.ylabel('Residual (bin avg.)')\nplt.xlabel('Distance (bin avg.)')",
      "outputs": [
        {
          "type": "ImageObject",
          "format": "image/png",
          "contentUrl": ""
        }
      ]
    },
    {
      "type": "Heading",
      "depth": 2,
      "content": [
        "Model 5: log-scaling arsenic"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "The binned residual plot indicates some nonlinearity in the arsenic variable. Note how the model over-estimated for low arsenic and underestimates for high arsenic. This suggests a log transformation or something similar."
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "We can again do this transformation right in the formula."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 15
      },
      "text": "model_form = ('switch ~ center(I(dist / 100.)) + center(np.log(arsenic)) + ' +\n              'center(I(educ / 4.)) + ' +\n              'center(I(dist / 100.)) : center(np.log(arsenic)) + ' + \n              'center(I(dist / 100.)) : center(I(educ / 4.)) + ' + \n              'center(np.log(arsenic)) : center(I(educ / 4.))'\n             )\n\nmodel5 = logit(model_form, df = df).fit()\nprint model5.summary()",
      "outputs": [
        "Optimization terminated successfully.\n         Current function value: 1931.554102\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 switch   No. Observations:                 3020\nModel:                          Logit   Df Residuals:                     3013\nMethod:                           MLE   Df Model:                            6\nDate:                Sat, 22 Dec 2012   Pseudo R-squ.:                 0.06192\nTime:                        13:05:57   Log-Likelihood:                -1931.6\nconverged:                       True   LL-Null:                       -2059.0\n                                        LLR p-value:                 3.517e-52\n==================================================================================================================\n                                                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------------------------------------------\nIntercept                                          0.3452      0.040      8.528      0.000         0.266     0.425\ncenter(I(dist / 100.))                            -0.9796      0.111     -8.809      0.000        -1.197    -0.762\ncenter(np.log(arsenic))                            0.9036      0.070     12.999      0.000         0.767     1.040\ncenter(I(educ / 4.))                               0.1785      0.039      4.577      0.000         0.102     0.255\ncenter(I(dist / 100.)):center(np.log(arsenic))    -0.1567      0.185     -0.846      0.397        -0.520     0.206\ncenter(I(dist / 100.)):center(I(educ / 4.))        0.3384      0.108      3.141      0.002         0.127     0.550\ncenter(np.log(arsenic)):center(I(educ / 4.))       0.0601      0.070      0.855      0.393        -0.078     0.198\n==================================================================================================================\n"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "And the binned residual plot for arsenic now looks better."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 16
      },
      "text": "arsenic_resids = bin_residuals(model5.resid, df['arsenic'], 40)\ndist_resids = bin_residuals(model5.resid, df['dist'], 40)\nplt.figure(figsize = (12, 5))\nplt.subplot(121)\nplot_binned_residuals(arsenic_resids)\nplt.ylabel('Residual (bin avg.)')\nplt.xlabel('Arsenic (bin avg.)')\nplt.subplot(122)\nplot_binned_residuals(dist_resids)\nplt.ylabel('Residual (bin avg.)')\nplt.xlabel('Distance (bin avg.)')",
      "outputs": [
        {
          "type": "ImageObject",
          "format": "image/png",
          "contentUrl": ""
        }
      ]
    },
    {
      "type": "Heading",
      "depth": 3,
      "content": [
        "Model error rates"
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "The ",
        {
          "type": "Code",
          "text": "pred_table()"
        },
        " gives us a confusion matrix for the model. We can use this to compute the error rate of the model."
      ]
    },
    {
      "type": "Paragraph",
      "content": [
        "We should compare this to the null error rates, which comes from a model that just classifies everything as whatever the most prevalent response is. Here 58% of the respondents were switchers, so the null model just classifies everyone as a switcher, and therefore has an error rate of 42%."
      ]
    },
    {
      "type": "CodeChunk",
      "meta": {
        "execution_count": 18
      },
      "text": "print model5.pred_table()\nprint 'Model Error rate: {0: 3.0%}'.format(\n    1 - np.diag(model5.pred_table()).sum() / model5.pred_table().sum())\nprint 'Null Error Rate: {0: 3.0%}'.format(\n    1 - df['switch'].mean())",
      "outputs": [
        "[[  568.   715.]\n [  387.  1350.]]\nModel Error rate:  36%\nNull Error Rate:  42%\n"
      ]
    }
  ]
}